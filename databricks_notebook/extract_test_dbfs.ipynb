{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a577f440-e798-4e41-9fcc-e4a40044954f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import when, col\n",
    "import pandas as pd\n",
    "\n",
    "def extract_spark():\n",
    "    url = \"https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/college-majors/grad-students.csv\"\n",
    "    FILESTORE_PATH = \"dbfs:/FileStore/mini_project11/\"  # Use /tmp as an alternative\n",
    "    dbfs_file_path = FILESTORE_PATH + \"/grad-students.csv\"\n",
    "    \n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"grade_student\").getOrCreate()\n",
    "    # Fetch the data\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    # Save the file locally in the cluster\n",
    "    local_path = \"/tmp/grad-students.csv\"\n",
    "    # Fetch data and save locally\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        df.to_csv(local_path, index=False)\n",
    "        print(f\"CSV saved locally at {local_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Check for permissions on DBFS\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(FILESTORE_PATH)\n",
    "        dbutils.fs.cp(f\"file:{local_path}\", dbfs_file_path)\n",
    "        print(f\"CSV successfully saved to {dbfs_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to DBFS: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert Pandas DataFrame to Spark DataFrame\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    \n",
    "    # To avoid mismatch between the schema of the existing Delta table and the DataFrame I am trying to append\n",
    "    spark.sql(\"DROP TABLE IF EXISTS grade_student_delta\")\n",
    "\n",
    "    # Write to Delta table (no need for explicit directory creation)\n",
    "    spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"grade_student_delta\")\n",
    "    print(\"Data successfully written to Delta table 'grade_student_delta'\")\n",
    "    \n",
    "    return dbfs_file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d61df4-e715-46d7-9719-7e64288ce491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "extract_spark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5909c481-c5a3-441b-8a5d-8e1c4b4b7adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/FileStore/mini_project11/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e068c5b7-d9ae-4691-b27c-1cc81772db9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def load(dataset=\"dbfs:/FileStore/mini_project11/grad-students.csv\"):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"grade_student\").getOrCreate()\n",
    "\n",
    "    # Read the dataset from DBFS\n",
    "    spark_df = spark.read.csv(dataset, header=True, inferSchema=True)\n",
    "    print(f\"Dataset loaded from {dataset}\")\n",
    "\n",
    "    # Drop the Delta table if it exists, when I don't use \"overwrite\"\n",
    "    spark.sql(\"DROP TABLE IF EXISTS grade_student_delta\")\n",
    "\n",
    "    # Write the Spark DataFrame to a Delta table\n",
    "    spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"grade_student_delta\")\n",
    "    print(\"Data successfully written to Delta table 'grade_student_delta'\")\n",
    "\n",
    "    if spark.catalog.tableExists(\"grade_student_delta\"):\n",
    "      print(\"Table exists. Proceeding with overwrite.\")\n",
    "    else:\n",
    "      print(\"Table does not exist. Creating a new one.\")\n",
    "      \n",
    "    # Print the number of rows in the DataFrame\n",
    "    nrows = spark_df.count()\n",
    "    print(f\"Number of rows in the dataset: {nrows}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9f8392-316c-4c43-8bcd-a6b469e0925e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6551535-6d21-4174-ae9e-eedc902d349b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def data_transform(table=\"grade_student_delta\"):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"grade_student\").getOrCreate()\n",
    "\n",
    "    # Define STEM categories\n",
    "    core_STEM = [\n",
    "        'Engineering',\n",
    "        'Computers & Mathematics',\n",
    "        'Biology & Life Science',\n",
    "        'Physical Sciences'\n",
    "    ]\n",
    "\n",
    "    other_STEM = [\n",
    "        'Agriculture & Natural Resources',\n",
    "        'Health',\n",
    "        'Interdisciplinary'\n",
    "    ]\n",
    "    \n",
    "    # Load the table into a DataFrame\n",
    "    sparktable = spark.table(table)\n",
    "    \n",
    "    # Add the \"STEM_major\" column based on conditions\n",
    "    sparktable = sparktable.withColumn(\n",
    "        \"STEM_major\",\n",
    "        when(col(\"Major_category\").isin(core_STEM), \"core_STEM\")\n",
    "        .when(col(\"Major_category\").isin(other_STEM), \"other_STEM\")\n",
    "        .otherwise(\"Other\")\n",
    "    )\n",
    "\n",
    "    # Overwrite the Delta table with schema evolution\n",
    "    sparktable.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(table)\n",
    "    print(f\"Table '{table}' updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44aaabb1-b609-4667-b2c0-be0ebf1288e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2777dc6-a4e9-459f-9f98-1900e5e4c1df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"grade_student\").getOrCreate()\n",
    "\n",
    "query_sample =\"\"\"\n",
    "            SELECT \n",
    "                Major_category,\n",
    "                SUM(Nongrad_employed) AS Total_Nongrad_employed,\n",
    "                SUM(Grad_employed) AS Total_Grad_employed,\n",
    "                SUM(Grad_unemployed) AS Total_Grad_unemployed,\n",
    "                SUM(Nongrad_unemployed) AS Total_Nongrad_unemployed,\n",
    "                SUM(Grad_total) AS Total_Grad_total,\n",
    "                SUM(Nongrad_total) AS Total_Nongrad_total\n",
    "            FROM grade_student_delta\n",
    "            GROUP BY Major_category\n",
    "            HAVING Total_Grad_employed + Total_Nongrad_employed > 10000\n",
    "            ORDER BY Total_Grad_employed + Total_Nongrad_employed DESC\n",
    "        \"\"\"\n",
    "\n",
    "query_result=spark.sql(query_sample)\n",
    "\n",
    "query_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4852afbb-ac38-4478-b068-f2d460cb97ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write a SQL query to the file\n",
    "with open(\"sample_query.sql\", \"w\") as file:\n",
    "    file.write(\"SELECT * FROM grade_student_delta\")\n",
    "\n",
    "# Read the query from the file and execute it\n",
    "with open(\"sample_query.sql\", \"r\") as file:\n",
    "    sql_query = file.read()\n",
    "\n",
    "query_result = spark.sql(sql_query)\n",
    "query_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511a66c9-3144-48cc-aaff-9221b8c159d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from delta.tables import DeltaTable\n",
    "print(\"Libraries are installed correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c19921d-f728-4972-baf1-0dea2196a8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "user_name = os.environ.get('USER')\n",
    "print(f\"Running as user: {user_name}\")\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "print(spark.conf.get(\"pipeline.storageLocation\", \"No storage location set\"))\n",
    "spark.conf.set(\"pipeline.storageLocation\", \"dbfs:/FileStore/mini_project11/\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "extract_test_dbfs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
